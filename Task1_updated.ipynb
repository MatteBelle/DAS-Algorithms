{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cip/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse\n",
    "from ipywidgets import interact, widgets, interact_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Z va reso D dimensionale (FATTO)\n",
    "# Calcolare la norma del gradiente come norma delle somme (FATTO NEL PUNTO 1, MANCA NEL RESTO)\n",
    "# 1.2 Per i dataset usare delle curve (cerchio, ellisse, etc) (FATTO)\n",
    "# 1.3 Utilizzare gli stessi dataset (FATTO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy version >=1.17.3 and <1.25.0 is required for this version of SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "NN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_types = {'cycle': nx.cycle_graph(NN),\n",
    "               'star': nx.star_graph(NN - 1),\n",
    "               'wheel': nx.wheel_graph(NN),\n",
    "               'complete': nx.complete_graph(NN),\n",
    "               'path': nx.path_graph(NN)\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_fn(z, q, r):\n",
    "    #return 0.5 * q * z * z + r * z, q * z + r\n",
    "    # returns the value of the quadratic function and its gradients\n",
    "    return 0.5 * z[0] * z[0] * q + z[1] * z[1] * q + z[0] * z[1] * q + z[0] * r + z[1] * r, z[0] * q + z[1] * q + r, 2*z[1]*q + z[1]*q + r\n",
    "\n",
    "Q = np.random.uniform(size=(NN))\n",
    "R = np.random.uniform(size=(NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_tracking(selected_graph):\n",
    "\n",
    "    # Init Adjacency matrix and Identity matrix\n",
    "    Adj = nx.adjacency_matrix(graph_types[selected_graph]).toarray()\n",
    "    I_NN = np.eye(NN)\n",
    "\n",
    "    # visualize the graph\n",
    "    plt.figure()\n",
    "    nx.draw(graph_types[selected_graph], with_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Init weights matrix\n",
    "    AA = np.zeros(shape=(NN, NN))\n",
    "    for ii in range(NN):\n",
    "        N_ii = np.nonzero(Adj[ii])[0]\n",
    "        deg_ii = len(N_ii)\n",
    "        for jj in N_ii:\n",
    "            deg_jj = len(np.nonzero(Adj[jj])[0])\n",
    "            AA[ii, jj] = 1 / (1 + max([deg_ii, deg_jj]))\n",
    "\n",
    "    AA += I_NN - np.diag(np.sum(AA, axis=0))\n",
    "\n",
    "    if 0:\n",
    "        print(np.sum(AA, axis=0))\n",
    "        print(np.sum(AA, axis=1))\n",
    "    \n",
    "    # Init variables for the optimization\n",
    "    MAXITERS = 1000\n",
    "    dd = 2\n",
    "\n",
    "    # ZZ is D-dimensional\n",
    "    ZZ_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    # SS is D-dimensional\n",
    "    SS_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    for ii in range(NN):\n",
    "        _ , SS_gt[0, ii, :-1], SS_gt[0, ii, -1] = quadratic_fn(ZZ_gt[0, ii], Q[ii], R[ii])\n",
    "\n",
    "    cost_gt = np.zeros((MAXITERS))\n",
    "    gradients_norm = np.zeros((MAXITERS, dd))\n",
    "    alpha = 1e-2\n",
    "    grad_ell_ii_new = np.zeros((dd))\n",
    "    grad_ell_ii_old = np.zeros((dd))\n",
    "\n",
    "    for kk in range(MAXITERS - 1):\n",
    "\n",
    "        # gradient tracking\n",
    "        for ii in range(NN):\n",
    "            N_ii = np.nonzero(Adj[ii])[0]\n",
    "\n",
    "            ZZ_gt[kk + 1, ii] += AA[ii, ii] * ZZ_gt[kk, ii]\n",
    "            SS_gt[kk + 1, ii] += AA[ii, ii] * SS_gt[kk, ii]\n",
    "            for jj in N_ii:\n",
    "                ZZ_gt[kk + 1, ii] += AA[ii, jj] * ZZ_gt[kk, jj]\n",
    "                SS_gt[kk + 1, ii] += AA[ii, jj] * SS_gt[kk, jj]\n",
    "\n",
    "            ZZ_gt[kk + 1, ii] -= alpha * SS_gt[kk, ii]\n",
    "\n",
    "            # print(Q[ii])\n",
    "            _, grad_ell_ii_new[:-1], grad_ell_ii_new[-1] = quadratic_fn(ZZ_gt[kk + 1, ii], Q[ii], R[ii])\n",
    "            _, grad_ell_ii_old[:-1], grad_ell_ii_old[-1] = quadratic_fn(ZZ_gt[kk, ii], Q[ii], R[ii])\n",
    "            SS_gt[kk + 1, ii] += grad_ell_ii_new - grad_ell_ii_old\n",
    "\n",
    "            gradients_norm[kk, :-1] = grad_ell_ii_new[:-1] - grad_ell_ii_old[:-1]\n",
    "            gradients_norm[kk, -1] = grad_ell_ii_new[-1] - grad_ell_ii_old[-1]\n",
    "\n",
    "            ell_ii_gt, _, _= quadratic_fn(ZZ_gt[kk, ii], Q[ii], R[ii])\n",
    "            cost_gt[kk] += ell_ii_gt\n",
    "\n",
    "    if 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(MAXITERS), ZZ_gt)\n",
    "        ax.grid()\n",
    "\n",
    "\n",
    "    ZZ_opt = -np.sum(R) / np.sum(Q)\n",
    "    opt_cost = 0.5 * np.sum(Q) * ZZ_opt**2 + np.sum(R) * ZZ_opt\n",
    "    # print(opt_cost)\n",
    "    # print(cost[-2])\n",
    "    # print(cost_gt[-2])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS - 1), np.abs(cost_gt[:-1] - opt_cost))\n",
    "    ax.grid()\n",
    "    plt.title('Cost function')\n",
    "    plt.show()\n",
    "\n",
    "    grad_norm = np.linalg.norm(gradients_norm, axis=1)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS), grad_norm)\n",
    "    ax.grid()\n",
    "    plt.title('Gradient norm')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19689184b25b41f0b3b0a78acb48045c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Graph type:', options=('cycle', 'star', 'wheel', 'complete', 'pathâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.gradient_tracking(selected_graph)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(gradient_tracking, selected_graph=widgets.Dropdown(options=graph_types.keys(), value='cycle', description='Graph type:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (100, 2)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "M = 100  # Number of points\n",
    "d = 2    # Dimension of the input space\n",
    "q = 4\n",
    "\n",
    "# Step 1: Generate a dataset\n",
    "X = np.random.randn(M, d)\n",
    "print(\"Shape of the dataset: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the nonlinear transformation function phi\n",
    "def phi(D):\n",
    "    return np.array([D[0], D[1], D[0]**2, D[1]**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate curves for labeling\n",
    "shapes = {\n",
    "    'circle': np.array([0, 0, 1, 1]),\n",
    "    'ellipse': np.array([0, 0, 0.5, 1]),\n",
    "    'parabola': np.array([0.8, -1, 0.5, 0]),\n",
    "    'hyperbola': np.array([0, 0, 0.5, -1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point labeling based on the curve and bias values\n",
    "def label_point(phi_x, true_w, true_b):\n",
    "    #print(np.dot(true_w, phi_x) + true_b)\n",
    "    return 1 if np.dot(true_w, phi_x) + true_b >= 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement Gradient Descent for Logistic Regression\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Logistic regression cost function\n",
    "def logistic_cost(w, b, Phi_X, labels):\n",
    "    m = len(labels)\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        total_cost += np.log(1 + np.exp(-labels[i] * z))\n",
    "        #print(total_cost)\n",
    "    return total_cost / m\n",
    "\n",
    "\n",
    "# Gradient of the cost function\n",
    "def compute_gradients(w, b, Phi_X, labels):\n",
    "    m = len(labels)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        p = sigmoid(z)\n",
    "        dw += (p - (labels[i] == 1)) * Phi_X[i]\n",
    "        db += (p - (labels[i] == 1))\n",
    "    return dw / m, db / m\n",
    "\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "def gradient_descent(Phi_X, labels, alpha, num_iterations):\n",
    "    w = np.random.randn(q)\n",
    "    b = np.random.randn()\n",
    "    costs = []\n",
    "    gradient_norms = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dw, db = compute_gradients(w, b, Phi_X, labels)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "\n",
    "        cost = logistic_cost(w, b, Phi_X, labels)\n",
    "        gradient_norm = np.append(dw, db)\n",
    "        costs.append(cost)\n",
    "        gradient_norms.append(gradient_norm)\n",
    "\n",
    "    return w, b, costs, gradient_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Plot results\n",
    "def plot_results(costs, gradient_norms):\n",
    "    iterations = len(costs)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(iterations), costs, label='Cost Function')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Evolution of Cost Function')\n",
    "    plt.legend()\n",
    "\n",
    "    grad_norm = np.linalg.norm(gradient_norms, axis=1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(iterations), grad_norm, label='Norm of Gradient')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Evolution of Gradient Norm')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the dataset\n",
    "Phi_X = np.array([phi(x) for x in X])\n",
    "\n",
    "# Label generation, visualisation and centralized classification\n",
    "\n",
    "def centralized_classification(selected_curve):\n",
    "    true_w = shapes[selected_curve]\n",
    "    true_b = np.random.randn()\n",
    "    labels = np.array([label_point(phi_x, true_w, true_b) for phi_x in Phi_X])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    # plot the curve\n",
    "    x1 = np.linspace(-3, 3, 100)\n",
    "    x2 = np.linspace(-3, 3, 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    F = true_w[0] * X1 + true_w[1] * X2 + true_w[2] * X1**2 + true_w[3] * X2**2 + true_b\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Dataset with Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for gradient descent\n",
    "    alpha = 0.1\n",
    "    num_iterations = 200\n",
    "\n",
    "    # Run gradient descent\n",
    "    w, b, costs, gradient_norms = gradient_descent(Phi_X, labels, alpha, num_iterations)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(costs, gradient_norms)\n",
    "\n",
    "    # Predicted labels\n",
    "    predicted_labels = np.array([1 if np.dot(w, phi_x) + b >= 0 else -1 for phi_x in Phi_X])\n",
    "\n",
    "    # Visualization of predicted labels\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(M):\n",
    "        if predicted_labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Predicted Labels')\n",
    "\n",
    "    # Real labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Real Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac68338301fa4b75b16a36ac27f64250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Curve:', options=('circle', 'ellipse', 'parabola', 'hyperbola'), vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.centralized_classification(selected_curve)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(centralized_classification, selected_curve=widgets.Dropdown(options=shapes.keys(), value='circle', description='Curve:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "M = 100  # Number of points\n",
    "d = 2    # Dimension of the input space\n",
    "q = 4\n",
    "\n",
    "# Step 1: Generate a dataset\n",
    "X = np.random.randn(M, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cost function\n",
    "def fn(z, Phi_X, labels):\n",
    "    w = z[:-1]\n",
    "    b = z[-1]\n",
    "    m = len(labels)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        p = sigmoid(z)\n",
    "        dw += (p - (labels[i] == 1)) * Phi_X[i]\n",
    "        db += (p - (labels[i] == 1))\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        total_cost += np.log(1 + np.exp(-labels[i] * z))\n",
    "    return total_cost / m, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the dataset\n",
    "Phi_X = np.array([phi(x) for x in X])\n",
    "\n",
    "# Label generation, visualisation and distibuted classification\n",
    "def distributed_classification(selected_shape, selected_graph):\n",
    "    true_w = shapes[selected_shape]\n",
    "    true_b = np.random.randn()\n",
    "    labels = np.array([label_point(phi_x, true_w, true_b) for phi_x in Phi_X])\n",
    "    \n",
    "    # Visualization of labeled points\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    # plot the curve\n",
    "    x1 = np.linspace(-3, 3, 100)\n",
    "    x2 = np.linspace(-3, 3, 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    F = true_w[0] * X1 + true_w[1] * X2 + true_w[2] * X1**2 + true_w[3] * X2**2 + true_b\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Dataset with Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for distributed gradient tracking\n",
    "    NN = 10\n",
    "    Phi_X_n = np.array_split(Phi_X, NN)\n",
    "    labels_n = np.array_split(labels, NN)\n",
    "    I_NN = np.eye(NN)\n",
    "    Adj = nx.adjacency_matrix(graph_types[selected_graph]).toarray()\n",
    "\n",
    "    # Init weights matrix\n",
    "    AA = np.zeros(shape=(NN, NN))\n",
    "    for ii in range(NN):\n",
    "        N_ii = np.nonzero(Adj[ii])[0]\n",
    "        deg_ii = len(N_ii)\n",
    "        for jj in N_ii:\n",
    "            deg_jj = len(np.nonzero(Adj[jj])[0])\n",
    "            AA[ii, jj] = 1 / (1 + max([deg_ii, deg_jj]))\n",
    "\n",
    "    AA += I_NN - np.diag(np.sum(AA, axis=0))\n",
    "\n",
    "    if 0:\n",
    "        print(np.sum(AA, axis=0))\n",
    "        print(np.sum(AA, axis=1))\n",
    "\n",
    "    # Init variables for the optimization\n",
    "    W = np.random.randn(NN, q)\n",
    "    B = np.random.uniform(size=(NN))\n",
    "    MAXITERS = 1000\n",
    "    dd = 5\n",
    "\n",
    "    ZZ_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    SS_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    for ii in range(NN):\n",
    "        _, SS_gt[0, ii, :-1], SS_gt[0, ii, -1] = fn(ZZ_gt[0, ii], Phi_X_n[ii], labels_n[ii])\n",
    "\n",
    "    cost_gt = np.zeros((MAXITERS))\n",
    "    gradients_norm = np.zeros((MAXITERS))\n",
    "    alpha = 1e-2\n",
    "    \n",
    "    # Gradient tracking\n",
    "    grad_ell_ii_new = np.zeros(dd)\n",
    "    grad_ell_ii_old = np.zeros(dd)\n",
    "\n",
    "    for kk in range(MAXITERS - 1):\n",
    "\n",
    "    # gradient tracking\n",
    "        for ii in range(NN):\n",
    "            N_ii = np.nonzero(Adj[ii])[0]\n",
    "            ZZ_gt[kk + 1, ii] += AA[ii, ii] * ZZ_gt[kk, ii]\n",
    "            SS_gt[kk + 1, ii] += AA[ii, ii] * SS_gt[kk, ii]\n",
    "            for jj in N_ii:\n",
    "                ZZ_gt[kk + 1, ii] += AA[ii, jj] * ZZ_gt[kk, jj]\n",
    "                SS_gt[kk + 1, ii] += AA[ii, jj] * SS_gt[kk, jj]\n",
    "\n",
    "            ZZ_gt[kk + 1, ii] -= alpha * SS_gt[kk, ii]\n",
    "\n",
    "            # print(Q[ii])\n",
    "            _, grad_ell_ii_new[:-1], grad_ell_ii_new[-1] = fn(ZZ_gt[kk + 1, ii], Phi_X_n[ii], labels_n[ii])\n",
    "            _, grad_ell_ii_old[:-1], grad_ell_ii_old[-1] = fn(ZZ_gt[kk, ii], Phi_X_n[ii], labels_n[ii])\n",
    "            SS_gt[kk + 1, ii] += grad_ell_ii_new - grad_ell_ii_old\n",
    "\n",
    "            gradient_norm = np.linalg.norm(grad_ell_ii_new - grad_ell_ii_old)\n",
    "            gradients_norm[kk] = gradient_norm\n",
    "\n",
    "            ell_ii_gt, _, _ = fn(ZZ_gt[kk, ii], Phi_X_n[ii], labels_n[ii])\n",
    "            cost_gt[kk] += ell_ii_gt\n",
    "\n",
    "    if 0:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(np.arange(MAXITERS), ZZ_gt)\n",
    "        ax.grid()\n",
    "\n",
    "    ZZ_opt = -np.sum(R) / np.sum(Q)\n",
    "    opt_cost = 0.5 * np.sum(Q) * ZZ_opt**2 + np.sum(R) * ZZ_opt\n",
    "\n",
    "    # Plot cost function\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS - 1), np.abs(cost_gt[:-1] - opt_cost))\n",
    "    ax.grid()\n",
    "    plt.title('Cost function')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot gradient norm\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS), gradients_norm)\n",
    "    ax.grid()\n",
    "    plt.title('Gradient norm')\n",
    "    plt.show()\n",
    "\n",
    "    # Predicted labels\n",
    "    # print(ZZ_gt[kk])\n",
    "    res = np.mean(ZZ_gt[kk], axis=0)\n",
    "    w = res[:-1]\n",
    "    b = res[-1]\n",
    "\n",
    "    predicted_labels = np.array([1 if np.dot(w, phi_x) + b >= 0 else -1 for phi_x in Phi_X])\n",
    "\n",
    "    # Visualization of predicted labels\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(M):\n",
    "        if predicted_labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Predicted Labels')\n",
    "\n",
    "    # Real labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Real Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca80437538d471fa81f739219df041f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Curve:', options=('circle', 'ellipse', 'parabola', 'hyperbola'), vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.distributed_classification(selected_shape, selected_graph)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(distributed_classification, selected_shape=widgets.Dropdown(options=shapes.keys(), value='circle', description='Curve:'), selected_graph=widgets.Dropdown(options=graph_types.keys(), value='cycle', description='Graph type:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
