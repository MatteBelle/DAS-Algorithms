{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse # Numpy version >=1.17.3 and <1.25.0 is required for this version of SciPy\n",
    "from ipywidgets import interact, widgets, interact_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "NN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_types = {'cycle': nx.cycle_graph(NN),\n",
    "               'star': nx.star_graph(NN - 1),\n",
    "               'wheel': nx.wheel_graph(NN),\n",
    "               'complete': nx.complete_graph(NN),\n",
    "               'path': nx.path_graph(NN)\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_fn(z, q, r):\n",
    "    \"\"\"\n",
    "    Calculate the value of the quadratic function and its gradient.\n",
    "\n",
    "    Parameters:\n",
    "    z (np.array): The input vector (x1, x2).\n",
    "    q (np.array): The parameter vector for quadratic terms (1 element per agent)\n",
    "    r (np.array): The parameter vector for linear terms (1 element per agent).\n",
    "\n",
    "    Returns:\n",
    "    tuple: The value of the quadratic function and its gradients w.r.t. x1 and x2.\n",
    "    \"\"\"\n",
    "    # Value of the quadratic function\n",
    "    f_value = 0.5 * (q * z[0]**2 + 2 * q * z[0] * z[1] + q * z[1]**2) + r * z[0] + r * z[1]\n",
    "    \n",
    "    # Gradients w.r.t. x1 and x2\n",
    "    grad_x1 = q * z[0] + q * z[1] + r\n",
    "    grad_x2 = q * z[0] + q * z[1] + r\n",
    "    \n",
    "    return f_value, grad_x1, grad_x2\n",
    "\n",
    "\n",
    "def find_critical_points(q, r):\n",
    "    # Since the equations are dependent, we can set z0 = -r/q - z1\n",
    "    # Let's assume z0 = -r/(2*q) and z1 = -r/(2*q) to satisfy the constraint\n",
    "    z_critical = np.array([-r/(2*q), -r/(2*q)])\n",
    "    return z_critical\n",
    "\n",
    "Q = np.random.uniform(size=(NN))\n",
    "R = np.random.uniform(size=(NN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_tracking(selected_graph):\n",
    "\n",
    "    # Init Adjacency matrix and Identity matrix\n",
    "    Adj = nx.adjacency_matrix(graph_types[selected_graph]).toarray()\n",
    "    I_NN = np.eye(NN)\n",
    "\n",
    "    # visualize the graph\n",
    "    plt.figure()\n",
    "    nx.draw(graph_types[selected_graph], with_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "    # Init weights matrix\n",
    "    AA = np.zeros(shape=(NN, NN))\n",
    "    for ii in range(NN):\n",
    "        N_ii = np.nonzero(Adj[ii])[0]\n",
    "        deg_ii = len(N_ii)\n",
    "        for jj in N_ii:\n",
    "            deg_jj = len(np.nonzero(Adj[jj])[0])\n",
    "            AA[ii, jj] = 1 / (1 + max([deg_ii, deg_jj]))\n",
    "\n",
    "    AA += I_NN - np.diag(np.sum(AA, axis=0))\n",
    "\n",
    "    if 0:\n",
    "        print(np.sum(AA, axis=0))\n",
    "        print(np.sum(AA, axis=1))\n",
    "    \n",
    "    # Init variables for the optimization\n",
    "    MAXITERS = 1000\n",
    "    dd = 2\n",
    "\n",
    "    # ZZ is D-dimensional\n",
    "    ZZ_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    # SS is D-dimensional\n",
    "    SS_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    for ii in range(NN):\n",
    "        _ , SS_gt[0, ii, :-1], SS_gt[0, ii, -1] = quadratic_fn(ZZ_gt[0, ii], Q[ii], R[ii])\n",
    "\n",
    "    cost_gt = np.zeros((MAXITERS))\n",
    "    gradients = np.zeros((MAXITERS, dd))\n",
    "    gradients_norm = np.zeros((MAXITERS))\n",
    "    alpha = 1e-2\n",
    "    grad_ell_ii_new = np.zeros((dd))\n",
    "    grad_ell_ii_old = np.zeros((dd))\n",
    "\n",
    "    for kk in range(MAXITERS - 1):\n",
    "\n",
    "        # gradient tracking\n",
    "        for ii in range(NN):\n",
    "            N_ii = np.nonzero(Adj[ii])[0]\n",
    "\n",
    "            ZZ_gt[kk + 1, ii] += AA[ii, ii] * ZZ_gt[kk, ii]\n",
    "            SS_gt[kk + 1, ii] += AA[ii, ii] * SS_gt[kk, ii]\n",
    "            for jj in N_ii:\n",
    "                ZZ_gt[kk + 1, ii] += AA[ii, jj] * ZZ_gt[kk, jj]\n",
    "                SS_gt[kk + 1, ii] += AA[ii, jj] * SS_gt[kk, jj]\n",
    "\n",
    "            ZZ_gt[kk + 1, ii] -= alpha * SS_gt[kk, ii]\n",
    "\n",
    "            # print(Q[ii])\n",
    "            _, grad_ell_ii_new[:-1], grad_ell_ii_new[-1] = quadratic_fn(ZZ_gt[kk + 1, ii], Q[ii], R[ii])\n",
    "            _, grad_ell_ii_old[:-1], grad_ell_ii_old[-1] = quadratic_fn(ZZ_gt[kk, ii], Q[ii], R[ii])\n",
    "            SS_gt[kk + 1, ii] += grad_ell_ii_new - grad_ell_ii_old\n",
    "\n",
    "            gradients[kk, :-1] += grad_ell_ii_new[:-1] - grad_ell_ii_old[:-1]\n",
    "            gradients[kk, -1] += grad_ell_ii_new[-1] - grad_ell_ii_old[-1]\n",
    "\n",
    "            ell_ii_gt, _, _= quadratic_fn(ZZ_gt[kk, ii], Q[ii], R[ii])\n",
    "            cost_gt[kk] += ell_ii_gt\n",
    "            #print('Iteration: ', kk, ' Node: ', ii, ' Cost: ', ell_ii_gt)\n",
    "        \n",
    "        gradients_norm[kk] = np.linalg.norm(gradients[kk])\n",
    "\n",
    "    # return ZZ at each iteration\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(MAXITERS), ZZ_gt[:, :, 0])\n",
    "    ax.grid()\n",
    "    plt.title('Z_x: First component')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(np.arange(MAXITERS), ZZ_gt[:, :, 1])\n",
    "    ax.grid()\n",
    "    plt.title('Z_y: Second component')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    z_critical = find_critical_points(Q, R)[1]\n",
    "    # as q and r use np.sum to get the sum of all elements\n",
    "    print('Optimal solution: ', z_critical)\n",
    "    opt_cost, grad_1, grad_2 = quadratic_fn(z_critical, np.sum(Q), np.sum(R))\n",
    "    print('Derivative at optimal solution: ', grad_1, grad_2)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS - 1), np.abs(cost_gt[:-1] - opt_cost))\n",
    "    ax.grid()\n",
    "    plt.title('Cost function')\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS - 1), gradients_norm[:-1])\n",
    "    ax.grid()\n",
    "    plt.title('Gradient norm')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e011496557c44d65b0910f0c2ddf3d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Graph type:', options=('cycle', 'star', 'wheel', 'complete', 'pathâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.gradient_tracking(selected_graph)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(gradient_tracking, selected_graph=widgets.Dropdown(options=graph_types.keys(), value='cycle', description='Graph type:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset:  (100, 2)\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "M = 100  # Number of points\n",
    "d = 2    # Dimension of the input space\n",
    "q = 4\n",
    "\n",
    "# Step 1: Generate a dataset\n",
    "X = np.random.randn(M, d)\n",
    "print(\"Shape of the dataset: \", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the nonlinear transformation function phi\n",
    "def phi(D):\n",
    "    return np.array([D[0], D[1], D[0]**2, D[1]**2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate curves for labeling\n",
    "shapes = {\n",
    "    'circle': np.array([0, 0, 1, 1]),\n",
    "    'ellipse': np.array([0, 0, 0.5, 1]),\n",
    "    'parabola': np.array([0.8, -1, 0.5, 0]),\n",
    "    'hyperbola': np.array([0, 0, 0.5, -1])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point labeling based on the curve and bias values\n",
    "def label_point(phi_x, true_w, true_b):\n",
    "    #print(np.dot(true_w, phi_x) + true_b)\n",
    "    return 1 if np.dot(true_w, phi_x) + true_b >= 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Implement Gradient Descent for Logistic Regression\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Logistic regression cost function\n",
    "def logistic_cost(w, b, Phi_X, labels):\n",
    "    m = len(labels)\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        total_cost += np.log(1 + np.exp(-labels[i] * z))\n",
    "        #print(total_cost)\n",
    "    return total_cost / m\n",
    "\n",
    "\n",
    "# Gradient of the cost function\n",
    "def compute_gradients(w, b, Phi_X, labels):\n",
    "    m = len(labels)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        p = sigmoid(z)\n",
    "        dw += (p - (labels[i] == 1)) * Phi_X[i]\n",
    "        db += (p - (labels[i] == 1))\n",
    "    return dw / m, db / m\n",
    "\n",
    "\n",
    "# Gradient Descent Algorithm\n",
    "def gradient_descent(Phi_X, labels, alpha, num_iterations):\n",
    "    w = np.random.randn(q)\n",
    "    b = np.random.randn()\n",
    "    costs = []\n",
    "    gradient_norms = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        dw, db = compute_gradients(w, b, Phi_X, labels)\n",
    "        w -= alpha * dw\n",
    "        b -= alpha * db\n",
    "\n",
    "        cost = logistic_cost(w, b, Phi_X, labels)\n",
    "        gradient_norm = np.append(dw, db)\n",
    "        costs.append(cost)\n",
    "        gradient_norms.append(gradient_norm)\n",
    "\n",
    "    return w, b, costs, gradient_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Plot results\n",
    "def plot_results(costs, gradient_norms):\n",
    "    iterations = len(costs)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.semilogy(range(iterations), costs, label='Cost')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Evolution of Cost Function')\n",
    "    plt.legend()\n",
    "\n",
    "    grad_norm = np.linalg.norm(gradient_norms, axis=1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.semilogy(range(iterations), grad_norm, label='Gradient Norm')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Gradient Norm')\n",
    "    plt.title('Evolution of Gradient Norm')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the dataset\n",
    "Phi_X = np.array([phi(x) for x in X])\n",
    "\n",
    "# Label generation, visualisation and centralized classification\n",
    "\n",
    "def centralized_classification(selected_curve):\n",
    "    true_w = shapes[selected_curve]\n",
    "    true_b = -np.random.rand()\n",
    "    labels = np.array([label_point(phi_x, true_w, true_b) for phi_x in Phi_X])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    # plot the curve\n",
    "    x1 = np.linspace(-3, 3, 100)\n",
    "    x2 = np.linspace(-3, 3, 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    F = true_w[0] * X1 + true_w[1] * X2 + true_w[2] * X1**2 + true_w[3] * X2**2 + true_b\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Dataset with Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for gradient descent\n",
    "    alpha = 0.1\n",
    "    num_iterations = 200\n",
    "\n",
    "    # Run gradient descent\n",
    "    w, b, costs, gradient_norms = gradient_descent(Phi_X, labels, alpha, num_iterations)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_results(costs, gradient_norms)\n",
    "\n",
    "    # Predicted labels\n",
    "    predicted_labels = np.array([1 if np.dot(w, phi_x) + b >= 0 else -1 for phi_x in Phi_X])\n",
    "\n",
    "    # Visualization of predicted labels\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(M):\n",
    "        if predicted_labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Predicted Labels')\n",
    "\n",
    "    # Real labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Real Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f0053dcc964af7ad22993d5e3d9be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Curve:', options=('circle', 'ellipse', 'parabola', 'hyperbola'), vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.centralized_classification(selected_curve)>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(centralized_classification, selected_curve=widgets.Dropdown(options=shapes.keys(), value='circle', description='Curve:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "M = 100  # Number of points\n",
    "d = 2    # Dimension of the input space\n",
    "q = 4\n",
    "\n",
    "# Step 1: Generate a dataset\n",
    "X = np.random.randn(M, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cost function\n",
    "def fn(z, Phi_X, labels):\n",
    "    w = z[:-1]\n",
    "    b = z[-1]\n",
    "    m = len(labels)\n",
    "    dw = np.zeros_like(w)\n",
    "    db = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        p = sigmoid(z)\n",
    "        dw += (p - (labels[i] == 1)) * Phi_X[i]\n",
    "        db += (p - (labels[i] == 1))\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(w, Phi_X[i]) + b\n",
    "        total_cost += np.log(1 + np.exp(-labels[i] * z))\n",
    "    return total_cost / m, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformation to the dataset\n",
    "Phi_X = np.array([phi(x) for x in X])\n",
    "\n",
    "# Label generation, visualisation and distibuted classification\n",
    "def distributed_classification(selected_shape, selected_graph):\n",
    "    true_w = shapes[selected_shape]\n",
    "    true_b = -np.random.rand()\n",
    "    labels = np.array([label_point(phi_x, true_w, true_b) for phi_x in Phi_X])\n",
    "    \n",
    "    # Visualization of labeled points\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    # plot the curve\n",
    "    x1 = np.linspace(-3, 3, 100)\n",
    "    x2 = np.linspace(-3, 3, 100)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    F = true_w[0] * X1 + true_w[1] * X2 + true_w[2] * X1**2 + true_w[3] * X2**2 + true_b\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Dataset with Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Parameters for distributed gradient tracking\n",
    "    NN = 10\n",
    "    Phi_X_n = np.array_split(Phi_X, NN)\n",
    "    labels_n = np.array_split(labels, NN)\n",
    "    I_NN = np.eye(NN)\n",
    "    Adj = nx.adjacency_matrix(graph_types[selected_graph]).toarray()\n",
    "\n",
    "    # Init weights matrix\n",
    "    AA = np.zeros(shape=(NN, NN))\n",
    "    for ii in range(NN):\n",
    "        N_ii = np.nonzero(Adj[ii])[0]\n",
    "        deg_ii = len(N_ii)\n",
    "        for jj in N_ii:\n",
    "            deg_jj = len(np.nonzero(Adj[jj])[0])\n",
    "            AA[ii, jj] = 1 / (1 + max([deg_ii, deg_jj]))\n",
    "\n",
    "    AA += I_NN - np.diag(np.sum(AA, axis=0))\n",
    "\n",
    "    if 0:\n",
    "        print(np.sum(AA, axis=0))\n",
    "        print(np.sum(AA, axis=1))\n",
    "\n",
    "    # Init variables for the optimization\n",
    "    W = np.random.randn(NN, q)\n",
    "    B = np.random.uniform(size=(NN))\n",
    "    MAXITERS = 1000\n",
    "    dd = 5\n",
    "\n",
    "    ZZ_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    SS_gt = np.zeros((MAXITERS, NN, dd))\n",
    "    for ii in range(NN):\n",
    "        _, SS_gt[0, ii, :-1], SS_gt[0, ii, -1] = fn(ZZ_gt[0, ii], Phi_X_n[ii], labels_n[ii])\n",
    "\n",
    "    cost_gt = np.zeros((MAXITERS))\n",
    "    gradients = np.zeros((MAXITERS, dd))\n",
    "    gradients_norm = np.zeros((MAXITERS, dd))\n",
    "    alpha = 1e-2\n",
    "    \n",
    "    # Gradient tracking\n",
    "    grad_ell_ii_new = np.zeros(dd)\n",
    "    grad_ell_ii_old = np.zeros(dd)\n",
    "\n",
    "    for kk in range(MAXITERS - 1):\n",
    "\n",
    "    # gradient tracking\n",
    "        for ii in range(NN):\n",
    "            N_ii = np.nonzero(Adj[ii])[0]\n",
    "            ZZ_gt[kk + 1, ii] += AA[ii, ii] * ZZ_gt[kk, ii]\n",
    "            SS_gt[kk + 1, ii] += AA[ii, ii] * SS_gt[kk, ii]\n",
    "            for jj in N_ii:\n",
    "                ZZ_gt[kk + 1, ii] += AA[ii, jj] * ZZ_gt[kk, jj]\n",
    "                SS_gt[kk + 1, ii] += AA[ii, jj] * SS_gt[kk, jj]\n",
    "\n",
    "            ZZ_gt[kk + 1, ii] -= alpha * SS_gt[kk, ii]\n",
    "\n",
    "            # print(Q[ii])\n",
    "            _, grad_ell_ii_new[:-1], grad_ell_ii_new[-1] = fn(ZZ_gt[kk + 1, ii], Phi_X_n[ii], labels_n[ii])\n",
    "            _, grad_ell_ii_old[:-1], grad_ell_ii_old[-1] = fn(ZZ_gt[kk, ii], Phi_X_n[ii], labels_n[ii])\n",
    "            SS_gt[kk + 1, ii] += grad_ell_ii_new - grad_ell_ii_old\n",
    "\n",
    "            gradients[kk, :-1] += grad_ell_ii_new[:-1] - grad_ell_ii_old[:-1]\n",
    "            gradients[kk, -1] += grad_ell_ii_new[-1] - grad_ell_ii_old[-1]\n",
    "\n",
    "            ell_ii_gt, _, _ = fn(ZZ_gt[kk, ii], Phi_X_n[ii], labels_n[ii])\n",
    "            cost_gt[kk] += ell_ii_gt\n",
    "    \n",
    "        gradients_norm[kk] = np.linalg.norm(gradients[kk])\n",
    "\n",
    "    # Plot cost function\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS - 1), cost_gt[:-1])\n",
    "    ax.grid()\n",
    "    plt.title('Cost function')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot gradient norm\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.semilogy(np.arange(MAXITERS - 1), gradients_norm[:-1])\n",
    "    ax.grid()\n",
    "    plt.title('Gradient norm')\n",
    "    plt.show()\n",
    "\n",
    "    # Predicted labels\n",
    "    # print(ZZ_gt[kk])\n",
    "    res = np.mean(ZZ_gt[kk], axis=0)\n",
    "    w = res[:-1]\n",
    "    b = res[-1]\n",
    "\n",
    "    predicted_labels = np.array([1 if np.dot(w, phi_x) + b >= 0 else -1 for phi_x in Phi_X])\n",
    "\n",
    "    # Visualization of predicted labels\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for i in range(M):\n",
    "        if predicted_labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Predicted Labels')\n",
    "\n",
    "    # Real labels\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for i in range(M):\n",
    "        if labels[i] == 1:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='b', marker='o')\n",
    "        else:\n",
    "            plt.scatter(X[i, 0], X[i, 1], color='r', marker='x')\n",
    "    plt.contour(X1, X2, F, [0], colors='k')\n",
    "    plt.grid()\n",
    "    plt.xlabel('x1')\n",
    "    plt.ylabel('x2')\n",
    "    plt.title('Real Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2e193780794c15a8047b958dd4de69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Curve:', options=('circle', 'ellipse', 'parabola', 'hyperbola'), vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.distributed_classification(selected_shape, selected_graph)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact_manual(distributed_classification, selected_shape=widgets.Dropdown(options=shapes.keys(), value='circle', description='Curve:'), selected_graph=widgets.Dropdown(options=graph_types.keys(), value='cycle', description='Graph type:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
